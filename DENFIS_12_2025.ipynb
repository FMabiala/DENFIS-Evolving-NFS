{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh-OvwMhOL2g"
      },
      "source": [
        "## **Name: Floyd Mabiala\"**\n",
        "## **Implementation of A Dynamic Evolving Neural Fuzzy Inference System**\n",
        "\n",
        "### Implementation of a DENFIS steps:\n",
        "\n",
        "  a. Build the Framework\n",
        "\n",
        "  b. Define & compute Gaussian member Function\n",
        "\n",
        "  c. Initial Predition using TSK\n",
        "\n",
        "  d. Online training\n",
        "  -  Rule creation (center, sigma, Consequence = [y_t, 0, 0 __init__]\n",
        "  -  Compute the distance (Euclidian distance)\n",
        "  -  Novelty detection (Threshold-based Method)\n",
        "  -  Update New rules: update (Center, sigma, consequence)\n",
        "\n",
        "  e. Prediction using K-nearest rules\n",
        "  \n",
        "  - Compute the didstance\n",
        "  - Compute the Activation\n",
        "  - Weighted average\n",
        "\n",
        "  f. Compute the Model prediction on Test Dataset\n",
        "  \n",
        "  g. Evaluate the Model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##References\n",
        "\n",
        "- Lughofer, Edwin. Evolving fuzzy systems-methodologies, advanced concepts and applications. Vol. 53. Berlin: Springer, 2011.\n",
        "\n",
        "- Kisi, Ozgur, Iman Mansouri, and Jong Wan Hu. \"A new method for evaporation modeling: Dynamic evolving neural‐fuzzy inference system.\" Advances in Meteorology 2017.1 (2017): 5356324.\n",
        "\n",
        "- Lughofer, Edwin, et al. \"Generalized smart evolving fuzzy systems.\" Evolving systems 6.4 (2015): 269-292.\n",
        "\n",
        "- Stock Trading: K. K. Ang and C. Quek, \"Stock Trading Using RSPOP: A Novel Rough Set-Based Neuro-Fuzzy Approach,\" in IEEE Transactions on Neural Networks, vol. 17, no. 5, pp. 1301-1315, Sept. 2006, doi: 10.1109/TNN.2006.875996.\n",
        "- DENFIS: N. K. Kasabov and Qun Song, \"DENFIS: dynamic evolving neural-fuzzy inference system and its application for time-series prediction,\" in IEEE Transactions on Fuzzy Systems, vol. 10, no. 2, pp. 144-154, April 2002, doi: 10.1109/91.995117."
      ],
      "metadata": {
        "id": "U8hoI5iHDe1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**DENFIS**\n",
        "In this project, **DENFIS** is the model that will be used to predict the Power Output of a Renewable Energy Source. DENFIS, or Dynamic Evolving Neural-Fuzzy Inference System, can be broadly characterized as an extension of ANFIS (Adaptive Neuro-Fuzzy Inference System) with the added capability of automatically generating membership functions. More formally, DENFIS is defined as an adaptive and intelligent computational framework designed to handle complex tasks such as time series prediction. Its distinguishing feature lies in its ability to construct and continuously refine a fuzzy inference system in real time, dynamically adjusting its structure and parameters based on the data encountered during the learning phase. This evolving nature enables DENFIS to respond effectively to non-stationary environments and incremental data streams, making it particularly suitable for applications requiring high adaptability and precision [Kasabov, 2002](https://ieeexplore.ieee.org/document/995117)\n"
      ],
      "metadata": {
        "id": "7rBPmpKrIAmU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox_tyMoHFo-8"
      },
      "source": [
        "## Dynamic Evolving Neural Fuzzy Inference System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WxGTHbLhQbsv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# DENFIS\n",
        "# Parameters: Maximum number of rules (m_max), distance threshold (theta), Number of nearest-neighbors (k), learning rate (lr), Sigma\n",
        "\n",
        "class DENFIS:\n",
        "    def __init__(self,\n",
        "                 m_max=60,\n",
        "                 distance_threshold=0.25,\n",
        "                 K=4,\n",
        "                 lr=0.06,\n",
        "                 init_sigma=0.3):\n",
        "\n",
        "        self.m_max = m_max\n",
        "        self.theta = distance_threshold\n",
        "        self.K = K\n",
        "        self.lr = lr\n",
        "        self.init_sigma = init_sigma\n",
        "\n",
        "        self.centers = []       # rule centers\n",
        "        self.sigmas = []        # widths\n",
        "        self.consequents = []   # linear TSK parameters: [w0, w1, w2] for y = w0 + w1*G + w2*T\n",
        "\n",
        "    # ---------------------------------------\n",
        "    # Gaussian membership: fn (x, c, sigma)\n",
        "    # ---------------------------------------\n",
        "    def gaussian(self, x, center, sigma):\n",
        "        return np.exp(-np.sum((x - center)**2) / (2 * sigma**2))\n",
        "\n",
        "    # ---------------------------------------==-------------------------------\n",
        "    # Takagi-Sugeno-Kang (TSK) rule output: here the consequence is a linear equation\n",
        "    # y = w_0 + w_1 x [0] + w_2 x [1] + ... + w_n x [n]\n",
        "    # The Conseq part of the TSK rule approximates the function locally around the C\n",
        "    # ---------------------------------------==--------------------------------\n",
        "    def tsk_output(self, x, params):\n",
        "        w0, w1, w2 = params\n",
        "        return w0 + w1 * x[0] + w2 * x[1]\n",
        "\n",
        "    # ---------------------------------------==-------------------------------\n",
        "    # Online training step. it handels both structure evolution (adding rules), &\n",
        "    # params updates\n",
        "    # ---------------------------------------==-------------------------------\n",
        "    def train(self, x_t, y_t):\n",
        "        x_t = np.array(x_t)\n",
        "\n",
        "        # -------------------------------\n",
        "        # If no rules → create first rule\n",
        "        # -------------------------------\n",
        "        if len(self.centers) == 0:\n",
        "            self.centers.append(x_t.copy())\n",
        "            self.sigmas.append(self.init_sigma)\n",
        "            # TSK params: bias, G coeff, T coeff\n",
        "            self.consequents.append(np.array([y_t, 0, 0], dtype=float))\n",
        "\n",
        "            # return y_t as a prediction for the first point\n",
        "            return y_t\n",
        "\n",
        "        # -------------------------------==-------------------------------------\n",
        "        # Compute distances to rule centers: Calculates the Euclidean disatnce x_t\n",
        "        # to all existing centers, & find the closest winning rule:\n",
        "        # index (i_best) - & - distance (d_best)\n",
        "        # -------------------------------==-------------------------------------\n",
        "        distances = [np.linalg.norm(x_t - c) for c in self.centers]\n",
        "        i_best = np.argmin(distances)\n",
        "        d_best = distances[i_best]\n",
        "\n",
        "        # -------------------------------==-----------------------------------\n",
        "        # Novelty detection & rule creation → new rule: Threshold-based Method\n",
        "        # -------------------------------==-----------------------------------\n",
        "\n",
        "        # if d_best>theta (novel pt) & rules<ma_max\n",
        "        if (d_best > self.theta) and (len(self.centers) < self.m_max):\n",
        "\n",
        "            self.centers.append(x_t.copy())\n",
        "            self.sigmas.append(self.init_sigma)\n",
        "            self.consequents.append(np.array([y_t, 0, 0], dtype=float))\n",
        "\n",
        "            return y_t\n",
        "\n",
        "        # -------------------------------==---------------\n",
        "        # Update the winning rule (if no new rules added)\n",
        "        # -------------------------------==---------------\n",
        "        c = self.centers[i_best]\n",
        "        s = self.sigmas[i_best]\n",
        "        w = self.consequents[i_best]\n",
        "\n",
        "        # Update center, x_t: new_center = old_center + lr * (x_t - old_center)\n",
        "        self.centers[i_best] = c + self.lr * (x_t - c)\n",
        "\n",
        "        # Update width (sigma): new_sigma = old_sigma + lr * (mean(|x_t-center|)-old_sigma)\n",
        "        self.sigmas[i_best] = s + self.lr * (np.abs(x_t - c).mean() - s)\n",
        "\n",
        "        # Update consequent (gradient descent)\n",
        "        #compute the output y_pred using TSK_ouput\n",
        "        y_pred_r = self.tsk_output(x_t, w)\n",
        "\n",
        "        error = y_t - y_pred_r\n",
        "\n",
        "        grad = np.array([1, x_t[0], x_t[1]])  # derivative with r.to TSK parameters\n",
        "\n",
        "\n",
        "        # Incremental learning (update):\n",
        "        # new_param = old_param + lr*err*grad (GD for LS minization)\n",
        "        self.consequents[i_best] = w + self.lr * error * grad\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # Prediction using DENFIS (K-nearest rules)\n",
        "        # -----------------------------------------\n",
        "        return self.predict(x_t)\n",
        "\n",
        "    # ---------------------------------------==-----------------------\n",
        "    # DENFIS prediction: K-nearest Rules, only the most relevant rules\n",
        "    # ---------------------------------------==-----------------------\n",
        "    def predict(self, x):\n",
        "        if len(self.centers) == 0:\n",
        "            return 0\n",
        "\n",
        "        x = np.array(x)\n",
        "\n",
        "        # Find K-nearest Rules: Compute distances\n",
        "        distances = np.array([np.linalg.norm(x - c) for c in self.centers])\n",
        "        idx = np.argsort(distances)[:self.K]\n",
        "\n",
        "        # Compute activations and weighted output\n",
        "        activations = np.array([self.gaussian(x, self.centers[i], self.sigmas[i])\n",
        "                                for i in idx]) # For each rule: active=gauss(x,c,s)\n",
        "\n",
        "        outputs = np.array([self.tsk_output(x, self.consequents[i])\n",
        "                            for i in idx]) # rule_output = tsk_output(x, conseq)\n",
        "\n",
        "        # Defuzzification\n",
        "        if np.sum(activations) == 0:\n",
        "            return outputs.mean()\n",
        "\n",
        "        return np.sum(activations * outputs) / np.sum(activations) #weighted average\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For PV Modeling:\n",
        "### Training DENFIS using PV data such as irradiance (G) & temperature (T) --> ref voltage Vref\n",
        "\n",
        "###Power Prediction Using DENFIS\n",
        "###A simulation of PV voltage control using DENFIS as power prediction model."
      ],
      "metadata": {
        "id": "f08HqH_PnJrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instatiating DENFIS\n",
        "\n",
        "denfis = DENFIS(\n",
        "    m_max=60,\n",
        "    distance_threshold=0.3,\n",
        "    K=4,\n",
        "    lr=0.055\n",
        ")\n",
        "\n",
        "# Simulated PV dataset\n",
        "for G, T, Vref in #dataset:   # dataset = list of (G, T, Vref)\n",
        "    denfis.train([G, T], Vref)\n"
      ],
      "metadata": {
        "id": "Bi_SwomKnBI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Make a Prediction for G and T"
      ],
      "metadata": {
        "id": "fGoOaU2moD0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G_new = 900     # W/m^2\n",
        "T_new = 35      # °C\n",
        "\n",
        "Vref_pred = denfis.predict([G_new, T_new])\n",
        "\n",
        "print(\"Predicted Reference Voltage:\", Vref_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktmaN0rHoCav",
        "outputId": "04999f57-1029-494a-d0f0-752fbe37a362"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Reference Voltage: 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}